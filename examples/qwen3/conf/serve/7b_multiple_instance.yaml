- serve_id: vllm_model
  engine: vllm
  engine_args:
    model: /models/Qwen3-8B
    gpu_memory_utilization: 0.9
    max_model_len: 32768
    max_num_seqs: 256
    trust_remote_code: true
    enable_chunked_prefill: true
  engine_args_specific:
    vllm:
      tensor_parallel_size: 1
      pipeline_parallel_size: 1
  resources:
    num_replicas: 8
    num_gpus: 1
  profile:
    input_len: 1024
    output_len: 1024
    num_prompts: 256