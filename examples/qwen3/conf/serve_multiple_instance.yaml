defaults:
  - _self_
  - serve: 7b_multiple_instance

experiment:
  exp_name: qwen3
  exp_dir: outputs/${experiment.exp_name}
  task:
    type: serve
    entrypoint: null
  deploy:
    port: 6701
    use_fs_serve: true
  runner:
    hostfile: null #examples/qwen3/conf/hostfile.txt # /path/to/hostfile.txt
    docker: cz-dev
    ssh_port: 22
    nnodes: 1
    nproc_per_node: 8
  auto_tuner:
    engines: [vllm]
    space:
      vllm:
        instance: [4, 8]
        tensor_model_parallel_size: [1, 2]
        pipeline_model_parallel_size: [1, 2]
        block_size: [16, 32]
        max_num_batched_tokens: [512, 1024, 2048]
        max_num_seqs: [64, 128, 256]
        #swap_space: [0, 4, 8]
    control:
      interval: 20
      run_best: False

  cmds:
    before_start: export RAY_DEDUP_LOGS=0 && source /root/miniconda3/bin/activate flagscale-inference

action: auto_tune

hydra:
  run:
    dir: ${experiment.exp_dir}/hydra